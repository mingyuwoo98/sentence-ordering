{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPIGZE78IVcsIr3pn1lxEor"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["## ReBERT Implementation\n","https://github.com/fabrahman/ReBART\n","\n","The implementation below is in reference to the paper above. It has been altered to work for our use case for the data challenge\n","\n","## CITATION:\n","\n","@inproceedings{Basu-brahman-chaturvedi-rebart,\n","    title = \"Is Everything in Order? A Simple Way to Order Sentences\",\n","    author = \"Somnath Basu Roy Chowdhury, Faeze Brahman and\n","      Snigdha Chaturvedi\",\n","    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP)\",\n","    month = nov,\n","    year = \"2021\",\n","    publisher = \"Association for Computational Linguistics\",\n","}\n","\n"],"metadata":{"id":"KjV-Re9AsoK7"}},{"cell_type":"code","source":["!pip3 install pickle5\n","!pip3 install absl-py==0.13.0\n","!pip3 install pandas==1.3.3\n","!pip3 install regex==2021.4.4\n","!pip3 install requests==2.26.0\n","!pip3 install rouge-score==0.0.4\n","!pip3 install sacremoses==0.0.45\n","!pip3 install scipy==1.7.0\n","!pip3 install sentence-transformers==2.0.0\n","!pip3 install sentencepiece==0.1.96\n","!pip3 install tokenizers==0.8.0rc4\n","!pip3 install torch==1.9.0\n","!pip3 install torchaudio==0.9.0\n","!pip3 install torchvision==0.10.0\n","!pip3 install tqdm==4.62.3\n","!pip3 install transformers==3.0.0"],"metadata":{"id":"Sye7v9hIHbI2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Import Packages"],"metadata":{"id":"xzjgohzppuhy"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"TGtE9IBFNbv2"},"outputs":[],"source":["# Packages\n","import numpy as np\n","import torch\n","from google.colab import drive\n","import pandas as pd\n","import os\n","import json\n","from tqdm import tqdm\n","import scipy\n","\n","import argparse\n","from torch.utils.data import Dataset\n","from torch.nn import CrossEntropyLoss\n","import torch.nn as nn\n","from transformers import AutoModelWithLMHead, AutoTokenizer\n","from torch.utils.data import RandomSampler, DataLoader\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","from tqdm import tqdm, trange"]},{"cell_type":"markdown","source":["# Set up Google Drive"],"metadata":{"id":"GjqPmNKEGIlR"}},{"cell_type":"code","source":["# Connect Google Drive where the data is located\n","drive.mount('/content/drive')"],"metadata":{"id":"5slksijzOBZ9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Train on GPU if applicable"],"metadata":{"id":"oJHdeDUvGKcQ"}},{"cell_type":"code","source":["# Check if GPU is active\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # check is GPU is available\n","\n","print(device)\n","!nvidia-smi -L"],"metadata":{"id":"tOunTewNOCx0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set seed for reproducibility\n","np.random.seed(940101)\n","torch.manual_seed(940101)"],"metadata":{"id":"opiouwyMOCvv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Read the Data"],"metadata":{"id":"sna5O0yfN8Rs"}},{"cell_type":"code","source":["FOLDER_PATH = '/content/drive/MyDrive/Colab Notebooks/data-challenge-2'\n","DATA_PATH = os.path.join(FOLDER_PATH, 'data')\n","TRAIN_JSONL_NAME = 'train_new.jsonl'\n","TEST_JSONL_NAME = 'test_new.jsonl'\n","GITHUB_JSONL_EXAMPLE_NAME = \"github_test_check.jsonl\"\n","\n","# Load the train and test pickled data\n","#   - key: id\n","#   - value: [list of sentences, array of ordering. ofsentences]\n","train_dict = pd.read_pickle(os.path.join(DATA_PATH, \"train.pickle\"))\n","test_dict = pd.read_pickle(os.path.join(DATA_PATH, \"test.pickle\"))"],"metadata":{"id":"KPH9y-KRN7n5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Match implementation input data\n","- The sample code requires JSONL file such that:\n","\n","  \n","[\n","    {\"orig_sents\": [orders...],\n","     \"shuf_sents\": [sentences...]\n","     }, ...\n","] \n"],"metadata":{"id":"wKn1KHwjGP-1"}},{"cell_type":"code","source":["def write_to_jsonl(data, sample_size = 10, is_test = False):\n","\n","    # If looking at only a sample, loop through first sample_size elements\n","    if not sample_size:\n","        sample_size = len(data)\n","\n","    df_dict = {}\n","    for i in range(sample_size):\n","        df_dict[i] = data[i]\n","\n","    to_convert_to_json = []\n","\n","    # Test dataset should have default empty set as orderings\n","    if not is_test:\n","        file_name = TRAIN_JSONL_NAME\n","        # Apply the same format as the example on github page\n","        for sentences, orders in df_dict.values():\n","            temp_dict = {}\n","            temp_dict[\"orig_sents\"] = [str(x) for x in orders]\n","            temp_dict[\"shuf_sents\"] = sentences\n","\n","            to_convert_to_json.append(temp_dict)\n","\n","    else:\n","        file_name = TEST_JSONL_NAME\n","        for sentences in df_dict.values():\n","            temp_dict = {}\n","            temp_dict[\"orig_sents\"] = []\n","            temp_dict[\"shuf_sents\"] = sentences[0]\n","            \n","            to_convert_to_json.append(temp_dict)\n","    \n","    # Convert to jsonl file\n","    with open(os.path.join(DATA_PATH, file_name), 'w') as f:\n","        for item in to_convert_to_json:\n","            f.write(json.dumps(item) + \"\\n\")\n","    pass"],"metadata":{"id":"F7GpUn4fnv_E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert train/test dictionary into jsonl format:\n","write_to_jsonl(train_dict, sample_size = None)\n","write_to_jsonl(test_dict, sample_size = None, is_test = True)\n","\n","# Compare with sample jsonl given in github repo\n","with open(os.path.join(DATA_PATH, GITHUB_JSONL_EXAMPLE_NAME)) as f:\n","    json_list = list(f)\n","\n","# Check train \n","with open(os.path.join(DATA_PATH, TRAIN_JSONL_NAME)) as f:\n","    train_json_list = list(f)\n","\n","# Check to see if it opens\n","with open(os.path.join(DATA_PATH, TEST_JSONL_NAME)) as f:\n","    test_json_list = list(f)\n","\n","# DEBUG LINE:\n","print(len(json_list), json_list[0])\n","print(len(train_json_list), train_json_list[0])\n","print(len(test_json_list), test_json_list[0])"],"metadata":{"id":"_-dIshZt2Ppi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def init_model(model_name, device, do_lower_case=False, args=None):\n","    # Initialize the REBART model and tokenizer\n","    tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=do_lower_case)\n","    model = AutoModelWithLMHead.from_pretrained(model_name)\n","\n","    # Make sure to send to GPU if applicable\n","    model.to(device)\n","    model.eval()\n","\n","    return tokenizer, model"],"metadata":{"id":"zYXkGUT723uf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_data(in_file, task=\"in_shuf\", is_test = False):\n","\n","    # Convert tthe jsonl files into list of tuples where:\n","    #   - (tokenized with special tokens, order of sentences, if applicable)\n","\n","    # Read each jsonl element and append to list\n","    all_lines = []\n","    with open(in_file, \"r\", encoding=\"utf-8\") as f:\n","        for line in f:\n","            all_lines.append(json.loads(line))\n","\n","    # index_with_sep will include the special tokens <S{i}> which indicates the start of a sentence.\n","    if not is_test:\n","        if task == \"index_with_sep\":\n","            examples = [\n","                (\n","                    f\"[shuffled] {' '.join([' '.join((f'<S{i}>', sent)) for i, sent in zip(list(range(len(line['orig_sents']))), line['shuf_sents'])])} [orig]\",\n","                    f\"{' '.join(line['orig_sents'])} <eos>\",\n","                )\n","            for line in all_lines\n","        ]\n","        else:\n","            examples = [\n","                (\n","                    f\"[shuffled] {line['shuf_sents'].rstrip(' <eos>') if type(line['shuf_sents']) == str else ' '.join(line['shuf_sents'])} [orig]\",\n","                    f\"{line['orig_sents'].rstrip(' <eos>') if type(line['orig_sents']) == str else ' '.join(line['orig_sents'])} <eos>\",\n","                )\n","                for line in all_lines\n","            ]\n","    else:\n","        if task == \"index_with_sep\":\n","            examples = [\n","                (\n","                    f\"[shuffled] {' '.join([' '.join((f'<S{i}>', sent)) for i, sent in zip(list(range(5)), line['shuf_sents'])])} [orig]\",\n","                )\n","            for line in all_lines\n","        ]\n","        else:\n","            examples = [\n","                (\n","                    f\"[shuffled] {line['shuf_sents'].rstrip(' <eos>') if type(line['shuf_sents']) == str else ' '.join(line['shuf_sents'])} [orig]\",\n","                )\n","                for line in all_lines\n","            ]\n","    return examples"],"metadata":{"id":"oaHCvNC66xYG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # DEBUG:\n","# in_file = os.path.join(DATA_PATH, GITHUB_JSONL_EXAMPLE_NAME)\n","# in_file_mike = os.path.join(DATA_PATH, TRAIN_JSONL_NAME)\n","# in_file_test_mike = os.path.join(DATA_PATH, TEST_JSONL_NAME)\n","\n","# examples_in_shuff = load_data(in_file, task=\"in_shuf\")\n","# examples_index_with_sep = load_data(in_file, task=\"index_with_sep\")\n","\n","# examples_in_shuff_mike = load_data(in_file_mike, task=\"in_shuf\")\n","# examples_index_with_sep_mike = load_data(in_file_mike, task=\"index_with_sep\")\n","\n","# examples_in_shuff_test_mike = load_data(in_file_test_mike, task=\"in_shuf\", is_test = True)\n","# examples_index_with_sep_test_mike = load_data(in_file_test_mike, task=\"index_with_sep\", is_test = True)\n","\n","# print(examples_in_shuff[0:1])\n","# print(examples_index_with_sep[0:1])\n","# print(examples_in_shuff_mike[0:1])\n","# print(examples_index_with_sep_mike[0:1])\n","# print(examples_in_shuff_test_mike[0:1])\n","# print(examples_index_with_sep_test_mike[0:1])"],"metadata":{"id":"h-rTRnPHIH1B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["FOLDER_PATH"],"metadata":{"id":"R7wjPxAHQEid"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize the args as mentioned in the paper\n","#   - NOTE: not all parameters will be useful in this data challenge\n","parser = argparse.ArgumentParser()\n","\n","# Required parameters\n","parser.add_argument('-f')\n","parser.add_argument(\"--out_dir\", default=FOLDER_PATH, type=str, help=\"Out directory for checkpoints.\")\n","\n","# Other parameters\n","parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\")\n","parser.add_argument(\"--do_eval\", default=False, help=\"Whether to run eval on the dev set.\")\n","parser.add_argument(\"--do_lower_case\", default=False, help=\"Set this flag if you are using an uncased model.\")\n","parser.add_argument(\"--do_train\", default=True, type=bool, help=\"Whether to run training.\")\n","parser.add_argument(\"--eval_batch_size\", default=16, type=int, help=\"Batch size for evaluation.\")\n","parser.add_argument(\"--eval_during_train\", default=False, help=\"Evaluate at each train logging step.\")\n","parser.add_argument(\"--gradient_accumulation_steps\", type=int, default=1, help=\"Steps before backward pass.\")\n","parser.add_argument(\"--learning_rate\", default=5e-6, type=float, help=\"The initial learning rate for Adam.\")\n","parser.add_argument(\"--logging_steps\", type=int, default=-1, help=\"Log every X updates steps (default after each epoch).\")\n","parser.add_argument(\"--max_input_length\", default=140, type=int, help=\"Maximum input event length in words.\")\n","parser.add_argument(\"--max_output_length\", default=120, type=int, help=\"Maximum output event length in words.\")\n","parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n","parser.add_argument(\"--max_steps\", default=1, type=int, help=\"If > 0: total number of training steps to perform.\")\n","parser.add_argument(\"--model_name_or_path\", default=\"facebook/bart-large-mnli\", type=str, help=\"LM checkpoint for initialization.\",)\n","parser.add_argument(\"--model_type\", default=\"\", type=str, help=\"which family of LM, e.g. gpt, gpt-xl, ....\",)\n","parser.add_argument(\"--num_train_epochs\", default=1.0, type=float, help=\"Number of training epochs to perform.\",)\n","parser.add_argument(\"--overwrite_cache\", action=\"store_true\", help=\"Overwrite the cached data.\")\n","parser.add_argument(\"--overwrite_out_dir\", action=\"store_true\", help=\"Overwrite the output directory.\",)\n","parser.add_argument(\"--continue_training\",action=\"store_true\",help=\"Continue training from the last checkpoint.\",)\n","parser.add_argument(\"--save_steps\", type=int, default=-1, help=\"Save checkpoint every X updates steps (default after each epoch).\",)\n","parser.add_argument(\"--save_total_limit\", type=int, default=None, help=\"Maximum number of checkpoints to keep\",)\n","parser.add_argument(\"--train_batch_size\", default=16, type=int, help=\"Batch size for training.\")\n","parser.add_argument(\"--train_file\", type=str, required=False, help=\"The input CSV train file.\")\n","parser.add_argument(\"--warmup_steps\", default=0, type=int, help=\"Linear warmup over warmup_steps.\")\n","parser.add_argument(\"--weight_decay\", default=0.0, type=float, help=\"Weight decay if we apply some.\")\n","parser.add_argument(\"--task\", type=str, help=\"what is the task?\")\n","args = parser.parse_args()"],"metadata":{"id":"-QaoNsWnDDP_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Prepare for Training"],"metadata":{"id":"73L2gkkDBJZC"}},{"cell_type":"code","source":["# Taken from GitHub\n","def get_loss(args, batch, model):\n","    \"\"\"\n","    Compute this batch loss\n","    \"\"\"\n","    input_ids = batch[\"inputs\"].to(device)\n","    input_mask = batch[\"input_mask\"].to(device)\n","    target_ids = batch[\"outputs\"].to(device)\n","    output_mask = batch[\"output_mask\"].to(device)\n","    decoder_input_ids = target_ids[:, :-1].contiguous()\n","\n","    # We don't send labels to model.forward because we want to compute per token loss\n","    lm_logits = model(\n","        input_ids, attention_mask=input_mask, decoder_input_ids=decoder_input_ids, use_cache=False\n","    )[0]  # use_cache=false is added for HF > 3.0\n","    batch_size, max_length, vocab_size = lm_logits.shape\n","\n","    # Compute loss for each instance and each token\n","    loss_fct = CrossEntropyLoss(reduction=\"none\")\n","    lm_labels = target_ids[:, 1:].clone().contiguous()\n","    lm_labels[target_ids[:, 1:] == args.pad_token_id] = -100\n","    loss = loss_fct(lm_logits.view(-1, vocab_size), lm_labels.view(-1)).view(\n","        batch_size, max_length\n","    )\n","\n","    # Only consider non padded tokens\n","    loss_mask = output_mask[..., :-1].contiguous()\n","    loss = torch.mul(loss_mask, loss)  # [batch_size, max_length]\n","    return loss"],"metadata":{"id":"XbEWiE22BJ7K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Taken from GitHub\n","def train(args, train_dataset, model, tokenizer, loss_fnc=get_loss, eval_dataset=None):\n","    \"\"\"\n","    Train the model.\n","    \"\"\" \n","    train_sampler = RandomSampler(train_dataset)\n","    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n","\n","    t_total = (len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs)\n","\n","    # Prepare optimizer and scheduler (linear warmup and decay)\n","    no_decay = [\"bias\", \"LayerNorm.weight\"]\n","    optimizer_grouped_parameters = [\n","        {\n","            \"params\": [\n","                p\n","                for n, p in model.named_parameters()\n","                if not any(nd in n for nd in no_decay)\n","            ],\n","            \"weight_decay\": args.weight_decay,\n","        },\n","        {\n","            \"params\": [\n","                p\n","                for n, p in model.named_parameters()\n","                if any(nd in n for nd in no_decay)\n","            ],\n","            \"weight_decay\": 0.0,\n","        },\n","    ]\n","\n","    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n","    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total)\n","\n","    # Train\n","    total_batch_size = args.train_batch_size * args.gradient_accumulation_steps\n","    global_step = 0\n","    epochs_trained = 0\n","    steps_trained_in_current_epoch = 0\n","\n","    tr_loss, logging_loss = 0.0, 0.0\n","\n","    model_to_resize = model.module if hasattr(model, \"module\") else model\n","    model_to_resize.resize_token_embeddings(len(tokenizer))\n","\n","    model.zero_grad()\n","    train_iterator = trange(epochs_trained, int(args.num_train_epochs), desc=\"Epoch\")\n","\n","    for _ in train_iterator:\n","        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\")\n","        for step, batch in enumerate(epoch_iterator):\n","\n","            model.train()\n","\n","            # Take the loss only for the part after the input (as in seq2seq architecture)\n","            loss = loss_fnc(args, batch, model)\n","            loss = loss.mean()\n","\n","            if args.gradient_accumulation_steps > 1:\n","                loss = loss / args.gradient_accumulation_steps\n","\n","            loss.backward()\n","\n","            tr_loss += loss.item()\n","            if (step + 1) % args.gradient_accumulation_steps == 0:\n","                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n","                optimizer.step()\n","                scheduler.step()  # Update learning rate schedule\n","                model.zero_grad()\n","                global_step += 1\n","\n","                if args.save_steps > 0 and global_step % args.save_steps == 0:\n","                    checkpoint_prefix = \"checkpoint\"\n","\n","                    # Save model checkpoint\n","                    out_dir = os.path.join(\n","                        args.out_dir, \"{}-{}\".format(checkpoint_prefix, global_step)\n","                    )\n","\n","                    if not os.path.exists(out_dir):\n","                        os.makedirs(out_dir)\n","\n","                    model_to_save = model.module if hasattr(model, \"module\") else model\n","                    model_to_save.save_pretrained(out_dir)\n","                    tokenizer.save_pretrained(out_dir)\n","                    torch.save(args, os.path.join(out_dir, \"training_args.bin\"))\n","\n","                    torch.save(\n","                        optimizer.state_dict(), os.path.join(out_dir, \"optimizer.pt\")\n","                    )\n","                    torch.save(\n","                        scheduler.state_dict(), os.path.join(out_dir, \"scheduler.pt\")\n","                    )\n","\n","    return global_step, tr_loss / global_step"],"metadata":{"id":"aAn0n33kBKPC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## PyTorch Custom Dataset\n"],"metadata":{"id":"Yms5z8NEDb2Y"}},{"cell_type":"code","source":["class EncoderDecoderTextDataset(Dataset):\n","    def __init__(self, tokenizer, args, file_path, block_size=512, is_test = False):\n","        # Load the JSONL file\n","        examples = load_data(file_path, task = \"index_with_sep\", is_test = is_test)\n","        # Input Data information (for both train/test):\n","        current_dict = {}\n","        inputs = [\n","            tokenizer.convert_tokens_to_ids(tokenizer.tokenize(ex[0]))\n","            for ex in examples\n","        ]\n","\n","        max_input_length = min(args.max_input_length, max([len(ex) for ex in inputs]))\n","        \n","        input_lengths = [min(len(ex), max_input_length) for ex in inputs]\n","\n","        inputs = [tokenizer.encode(\n","            ex[0], add_special_tokens=False, max_length=max_input_length, pad_to_max_length=True)\n","            for ex in examples]\n","\n","        current_dict[\"inputs\"] = inputs\n","        current_dict[\"input_lengths\"] = input_lengths\n","\n","        # Output Data information (only for train, test will have nothing):\n","        if not is_test:\n","            outputs = [\n","                [inputs[i][-1]]\n","                + tokenizer.convert_tokens_to_ids(tokenizer.tokenize(ex[1]))\n","                for i, ex in enumerate(examples)\n","            ]\n","            max_output_length = min(\n","                args.max_output_length, max([len(ex) for ex in outputs])\n","            )\n","            output_lengths = [min(len(ex), max_output_length) for ex in outputs]\n","\n","            current_dict[\"outputs\"] = outputs\n","            current_dict[\"output_lengths\"] = output_lengths\n","            outputs = [tokenizer.encode(\n","                '[orig] ' + ex[1], add_special_tokens=False, max_length=max_output_length, pad_to_max_length=True)\n","                for ex in examples]\n","\n","        self.examples = current_dict\n","\n","    def __len__(self):\n","        return len(self.examples[\"input_lengths\"])\n","\n","    def __getitem__(self, item):\n","        # Input Data information (for both train/test):\n","        current_dict = {}\n","        inputs = torch.tensor(self.examples[\"inputs\"][item])\n","        max_length = inputs.shape[0]\n","        input_lengths = self.examples[\"input_lengths\"][item]\n","        input_mask = torch.tensor([1] * input_lengths + [0] * (max_length - input_lengths))\n","\n","        current_dict[\"inputs\"] = inputs\n","        current_dict[\"input_mask\"] = input_mask\n","\n","        # Output Data information (only for train, test will have nothing):\n","        if self.examples.get(\"outputs\") != None:\n","            outputs = torch.tensor(self.examples[\"outputs\"][item])\n","            max_length = outputs.shape[0]\n","            output_lengths = self.examples[\"output_lengths\"][item]\n","            output_mask = torch.tensor([1] * output_lengths + [0] * (max_length - output_lengths))\n","            current_dict[\"outputs\"] = outputs\n","            current_dict[\"output_mask\"] = output_mask\n","\n","        return current_dict"],"metadata":{"id":"xC07kOeyLkEc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Begin Training"],"metadata":{"id":"4AQJkem5Q_lS"}},{"cell_type":"code","source":["# Initialize the tokenize and pretrained BART model\n","tokenizer, model = init_model(args.model_name_or_path, device=device, do_lower_case=args.do_lower_case)"],"metadata":{"id":"7MtObH5yQqQW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Apply the special tokens specified in the paper\n","args.pad_token_id = tokenizer.pad_token_id\n","args.block_size = tokenizer.max_len_single_sentence\n","\n","special_tokens = [\"[shuffled]\", \"[orig]\", \"<eos>\"]\n","extra_specials = [f\"<S{i}>\" for i in range(args.max_output_length)]\n","special_tokens += extra_specials\n","tokenizer.pad_token = \"<pad>\"\n","tokenizer.eos_token = \"<eos>\"\n","tokenizer.add_tokens(special_tokens)\n","model.resize_token_embeddings(len(tokenizer))"],"metadata":{"id":"dr-xRWLAGjPt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize the train PyTorch dataset\n","train_data = os.path.join(DATA_PATH, TRAIN_JSONL_NAME)\n","train_set = EncoderDecoderTextDataset(tokenizer, args, train_data)"],"metadata":{"id":"gNIlhP9ssPtc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Training\n","model.to(device) # send to GPU if applicable\n","\n","if args.do_train:\n","    global_step, tr_loss = train(\n","        args,\n","        train_set,\n","        model,\n","        tokenizer,\n","        loss_fnc=get_loss,\n","    )\n","\n","    # Create output directory if needed\n","    if not os.path.exists(args.out_dir):\n","        os.makedirs(args.out_dir)\n","\n","    # Save the following:\n","    #   - arguments\n","    #   - model\n","    #   - tokenizer\n","    model_to_save = model.module if hasattr(model, \"module\") else model\n","    model_to_save.save_pretrained(args.out_dir)\n","    tokenizer.save_pretrained(args.out_dir)\n","\n","    # Good practice: save your training arguments together with the trained model\n","    torch.save(args, os.path.join(args.out_dir, \"training_args.bin\"))"],"metadata":{"id":"jcBrIGkRMcle"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# For Prediction (trained models)"],"metadata":{"id":"UVYoG_Lq88vk"}},{"cell_type":"code","source":["# Reload the saved model and parameters\n","tokenizer, model = init_model(\n","    args.out_dir, device=device, do_lower_case=args.do_lower_case, args=args\n",")\n","args.block_size = tokenizer.max_len_single_sentence\n","model.to(device)"],"metadata":{"id":"A7ajwOKf895S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Prepare the test data\n","test_data = os.path.join(DATA_PATH, TEST_JSONL_NAME)\n","test_set = EncoderDecoderTextDataset(tokenizer, args, test_data, is_test = True)\n","test_loader = DataLoader(test_set, batch_size=1)"],"metadata":{"id":"oVt6phf0wxtD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["outputs = []\n","for i, batch in tqdm(enumerate(test_loader)):\n","    # Send the inputs to gpu\n","    input_ids = batch[\"inputs\"].to(device)\n","    model_outputs = model.generate(input_ids=input_ids).tolist()[0]\n","    # Decode to get position vectors (only elements 1 through 6)\n","    decoded = tokenizer.decode(model_outputs[1:6])\n","    decoded_list = decoded.split(\" \")\n","    pos_pred = []\n","    for item in decoded_list:\n","    # first element may be empty string\n","        if item != '':\n","            pos_pred.append(int(item))\n","    outputs.append(pos_pred)"],"metadata":{"id":"_ExDTL3vw9O9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_to_submit = pd.DataFrame(np.concatenate([np.arange(len(test_loader))[:, None], np.array(outputs)], axis = 1))\n","df_to_submit.columns = ['id'] + [f\"index_{i+1}\"for i in range(5)]\n","\n","# CHECK RESULTS\n","df_to_submit.head()"],"metadata":{"id":"Yq2Ns9w4BMvB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# SAVE SUBMISSION FOR KAGGLE\n","df_to_submit.to_csv(os.path.join(FOLDER_PATH, \"submission_mike.csv\"), index = False)"],"metadata":{"id":"8VSg1j0ODLzJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Submission should be: col. = id, index_1, ..., index_5\n","#   - double check if submission is in the same format\n","sample_submission = pd.read_csv(os.path.join(DATA_PATH, \"sample_submission.csv\"))\n","sample_submission"],"metadata":{"id":"Ns0xBVQ82QLU"},"execution_count":null,"outputs":[]}]}